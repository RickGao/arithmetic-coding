from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Set, Optional


class NGramModel:
    """
    N-gramè¯­è¨€æ¨¡å‹ç±»

    ç”¨äºä»ç´¢å¼•åºåˆ—ä¸­å­¦ä¹ n-gramæ¦‚ç‡åˆ†å¸ƒ
    æ”¯æŒå¼€å§‹ç¬¦ï¼ˆ-1ï¼‰å’Œç»“æŸç¬¦ï¼ˆ-2ï¼‰

    æ”¹è¿›ï¼šæ”¯æŒé¢„å®šä¹‰è¯æ±‡è¡¨ï¼ˆinitial_vocabå‚æ•°ï¼‰
    """

    def __init__(self,
                 n: int = 3,
                 k: float = 0.00001,
                 start_token: int = -1,
                 end_token: int = -2,
                 initial_vocab: Optional[Set[int]] = None):
        """
        åˆå§‹åŒ–N-gramæ¨¡å‹

        å‚æ•°:
            n: n-gramçš„nå€¼ï¼ˆä¾‹å¦‚ï¼Œn=3è¡¨ç¤ºtrigramï¼‰
            k: å¹³æ»‘å‚æ•°ï¼ˆåŠ æ³•å¹³æ»‘/Laplaceå¹³æ»‘ï¼‰
            start_token: å¼€å§‹ç¬¦çš„ç´¢å¼•å€¼ï¼ˆé»˜è®¤-1ï¼‰
            end_token: ç»“æŸç¬¦çš„ç´¢å¼•å€¼ï¼ˆé»˜è®¤-2ï¼‰
            initial_vocab: é¢„å®šä¹‰çš„å®Œæ•´è¯æ±‡è¡¨ï¼ˆå¯é€‰ï¼‰
                          å¦‚æœæä¾›ï¼Œä¼šç¡®ä¿è¿™äº›ç¬¦å·éƒ½åœ¨æœ€ç»ˆè¯æ±‡è¡¨ä¸­
                          å³ä½¿å®ƒä»¬åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡å‡ºç°

        ç¤ºä¾‹:
            # ä¸æŒ‡å®šinitial_vocab - å‘åå…¼å®¹ï¼Œè¡Œä¸ºå®Œå…¨ç›¸åŒ
            model = NGramModel(n=3, k=0.00001)

            # æŒ‡å®šinitial_vocab - ç¡®ä¿æ‰€æœ‰ç¬¦å·éƒ½å¯ç¼–ç 
            model = NGramModel(n=3, k=0.00001, initial_vocab=set(range(2048)))
        """
        self.n = n
        self.k = k
        self.start_token = start_token
        self.end_token = end_token
        self.initial_vocab = initial_vocab  # æ–°å¢ï¼šé¢„å®šä¹‰è¯æ±‡è¡¨
        self.vocab = set()  # è¯æ±‡è¡¨
        self.ngram_counts = Counter()  # n-gramè®¡æ•°
        self.context_counts = Counter()  # å‰n-1ä¸ªå­—ç¬¦çš„è®¡æ•°
        self.prob_distribution = {}  # æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ

    def fit(self, sequences: List[List[int]]):
        """
        è®­ç»ƒæ¨¡å‹

        å‚æ•°:
            sequences: è¾“å…¥çš„å¤šè¡Œåºåˆ—ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªlist of indices
                      ä¾‹å¦‚: [[1, 2, 3, 4], [2, 3, 4, 5, 6], ...]

        æ³¨æ„:
            - ä¼šè‡ªåŠ¨åœ¨æ¯ä¸ªåºåˆ—å¼€å¤´æ·»åŠ  n-1 ä¸ªå¼€å§‹ç¬¦ï¼ˆé»˜è®¤-1ï¼‰
            - ä¼šè‡ªåŠ¨åœ¨æ¯ä¸ªåºåˆ—ç»“å°¾æ·»åŠ  1 ä¸ªç»“æŸç¬¦ï¼ˆé»˜è®¤-2ï¼‰
            - å¦‚æœæä¾›äº†initial_vocabï¼Œä¼šç¡®ä¿æ‰€æœ‰é¢„å®šä¹‰ç¬¦å·éƒ½åœ¨è¯æ±‡è¡¨ä¸­
        """
        # é¢„å¤„ç†åºåˆ—ï¼šæ·»åŠ å¼€å§‹ç¬¦å’Œç»“æŸç¬¦
        processed_sequences = []
        for seq in sequences:
            # åœ¨å¼€å¤´æ·»åŠ  n-1 ä¸ªå¼€å§‹ç¬¦
            padded_seq = [self.start_token] * (self.n - 1) + seq + [self.end_token]
            processed_sequences.append(padded_seq)

        # åˆå¹¶æ‰€æœ‰åºåˆ—æ¥ç»Ÿè®¡ï¼ˆåŒ…æ‹¬ç‰¹æ®Šç¬¦å·ï¼‰
        all_indices = []
        for seq in processed_sequences:
            all_indices.extend(seq)

        # æ„å»ºè¯æ±‡è¡¨
        if self.initial_vocab is not None:
            # å¦‚æœæä¾›äº†é¢„å®šä¹‰è¯æ±‡è¡¨ï¼Œä½¿ç”¨å®ƒä½œä¸ºåŸºç¡€
            self.vocab = set(self.initial_vocab)
            # åŒæ—¶åŒ…å«è®­ç»ƒæ•°æ®ä¸­çš„ç¬¦å·ï¼ˆç¡®ä¿ä¸é—æ¼ï¼‰
            self.vocab.update(all_indices)
        else:
            # å¦åˆ™åªä½¿ç”¨è®­ç»ƒæ•°æ®ä¸­çš„ç¬¦å·ï¼ˆåŸå§‹è¡Œä¸ºï¼Œå‘åå…¼å®¹ï¼‰
            self.vocab = set(all_indices)

        V = len(self.vocab)

        # ç»Ÿè®¡n-gramï¼ˆåªç»Ÿè®¡å®é™…å‡ºç°çš„ï¼‰
        for seq in processed_sequences:
            for i in range(len(seq) - self.n + 1):
                ngram = tuple(seq[i:i + self.n])
                context = ngram[:-1]  # å‰n-1ä¸ªå…ƒç´ 

                self.ngram_counts[ngram] += 1
                self.context_counts[context] += 1

        # è®¡ç®—æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ
        # æ ¼å¼: {context: {next_char: probability}}
        self.prob_distribution = defaultdict(dict)

        # ä¸ºæ¯ä¸ªè§è¿‡çš„ä¸Šä¸‹æ–‡è®¡ç®—æ¦‚ç‡
        for context in self.context_counts.keys():
            context_count = self.context_counts[context]

            # å…³é”®æ”¹è¿›ï¼šä¸ºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªç¬¦å·è®¡ç®—æ¦‚ç‡
            for next_char in self.vocab:
                ngram = context + (next_char,)
                ngram_count = self.ngram_counts.get(ngram, 0)  # æœªè§è¿‡çš„ä¸º0

                # ä½¿ç”¨åŠ æ³•å¹³æ»‘ (Add-k smoothing / Laplace smoothing)
                numerator = ngram_count + self.k
                denominator = context_count + self.k * V

                probability = numerator / denominator
                self.prob_distribution[context][next_char] = probability

    def get_probability_distribution(self) -> Dict[Tuple, Dict[int, float]]:
        """
        è·å–å®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒå­—å…¸

        è¿”å›:
            å­—å…¸æ ¼å¼: {(å‰n-1ä¸ªå­—ç¬¦çš„tuple): {ä¸‹ä¸€ä¸ªå­—ç¬¦: æ¦‚ç‡}}
            ä¾‹å¦‚: {(1, 2): {3: 0.5, 4: 0.3, 5: 0.2}}
        """
        return dict(self.prob_distribution)

    def get_next_char_prob(self, context: Tuple[int, ...]) -> Dict[int, float]:
        """
        ç»™å®šcontextï¼Œè¿”å›ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ

        å‚æ•°:
            context: å‰n-1ä¸ªå­—ç¬¦ç»„æˆçš„tuple

        è¿”å›:
            å­—å…¸ {next_char: probability}

        ä¸Šä¸‹æ–‡å¤„ç†:
            - å¦‚æœcontextåœ¨è®­ç»ƒä¸­è§è¿‡ï¼šè¿”å›è®­ç»ƒå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒ
            - å¦‚æœcontextæœªè§è¿‡ï¼šè¿”å›å‡åŒ€åˆ†å¸ƒï¼ˆæ‰€æœ‰vocabç¬¦å·ç­‰æ¦‚ç‡ï¼‰
        """
        if context in self.prob_distribution:
            # ä¸Šä¸‹æ–‡è§è¿‡ï¼Œè¿”å›è®­ç»ƒçš„æ¦‚ç‡åˆ†å¸ƒ
            return self.prob_distribution[context]
        else:
            # ä¸Šä¸‹æ–‡æœªè§è¿‡ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
            V = len(self.vocab)
            return {char: 1.0 / V for char in self.vocab}

    def predict_next(self, context: Tuple[int, ...]) -> int:
        """
        ç»™å®šcontextï¼Œé¢„æµ‹æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå­—ç¬¦

        å‚æ•°:
            context: å‰n-1ä¸ªå­—ç¬¦ç»„æˆçš„tuple

        è¿”å›:
            æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå­—ç¬¦çš„index
        """
        prob_dist = self.get_next_char_prob(context)
        return max(prob_dist.items(), key=lambda x: x[1])[0]

    def get_probability(self, ngram: Tuple[int, ...]) -> float:
        """
        è®¡ç®—ç‰¹å®šn-gramçš„æ¦‚ç‡

        å‚æ•°:
            ngram: é•¿åº¦ä¸ºnçš„tuple

        è¿”å›:
            è¯¥n-gramçš„æ¡ä»¶æ¦‚ç‡
        """
        if len(ngram) != self.n:
            raise ValueError(f"ngramé•¿åº¦å¿…é¡»ä¸º{self.n}")

        context = ngram[:-1]
        next_char = ngram[-1]

        if context in self.prob_distribution:
            return self.prob_distribution[context].get(next_char, 0.0)
        else:
            # ä¸Šä¸‹æ–‡æœªè§è¿‡ï¼Œè¿”å›å‡åŒ€æ¦‚ç‡
            return 1.0 / len(self.vocab)

    def get_start_context(self) -> Tuple[int, ...]:
        """
        è·å–å¥å­å¼€å§‹æ—¶çš„contextï¼ˆn-1ä¸ªå¼€å§‹ç¬¦ï¼‰

        è¿”å›:
            å¼€å§‹contextçš„tuple
        """
        return tuple([self.start_token] * (self.n - 1))

    def is_end_token(self, token: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç»“æŸç¬¦

        å‚æ•°:
            token: è¦åˆ¤æ–­çš„token

        è¿”å›:
            æ˜¯å¦ä¸ºç»“æŸç¬¦
        """
        return token == self.end_token


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 70)
    print("NGramModel - å‘åå…¼å®¹ + æ–°å¢initial_vocabæ”¯æŒ")
    print("=" * 70)

    # ç¤ºä¾‹æ•°æ®
    sequences = [
        [1, 2, 3, 4, 5],
        [2, 3, 4, 5, 6],
        [1, 2, 3, 5, 6],
    ]

    # ========================================================================
    print("\nç¤ºä¾‹ 1: åŸå§‹ç”¨æ³•ï¼ˆå‘åå…¼å®¹ï¼Œè¡Œä¸ºå®Œå…¨ç›¸åŒï¼‰")
    print("=" * 70)

    model_old = NGramModel(n=3, k=0.01, start_token=-1, end_token=-2)
    model_old.fit(sequences)

    print(f"è¯æ±‡è¡¨å¤§å°: {len(model_old.vocab)}")
    print(f"è¯æ±‡è¡¨: {sorted(model_old.vocab)}")

    # æµ‹è¯•æœªè§ç¬¦å·
    test_seq = [1, 2, 100]  # 100æœªè§è¿‡
    missing = [s for s in test_seq if s not in model_old.vocab]
    if missing:
        print(f"âŒ æ— æ³•ç¼–ç ç¬¦å·: {missing}")

    # ========================================================================
    print("\nç¤ºä¾‹ 2: æ–°ç”¨æ³•ï¼ˆä½¿ç”¨initial_vocabï¼‰")
    print("=" * 70)

    # é¢„å®šä¹‰å®Œæ•´è¯æ±‡è¡¨
    full_vocab = set(range(200))  # 0-199

    model_new = NGramModel(
        n=3,
        k=0.01,
        start_token=-1,
        end_token=-2,
        initial_vocab=full_vocab  # ğŸ”‘ æ–°å¢å‚æ•°
    )
    model_new.fit(sequences)  # è®­ç»ƒæ•°æ®ä¸å˜

    print(f"è¯æ±‡è¡¨å¤§å°: {len(model_new.vocab)}")

    # æµ‹è¯•æœªè§ç¬¦å·
    missing = [s for s in test_seq if s not in model_new.vocab]
    if missing:
        print(f"âŒ æ— æ³•ç¼–ç ç¬¦å·: {missing}")
    else:
        print(f"âœ… æ‰€æœ‰ç¬¦å·éƒ½å¯ç¼–ç ï¼ˆåŒ…æ‹¬100ï¼‰")

    # ========================================================================
    print("\nç¤ºä¾‹ 3: ä¸Šä¸‹æ–‡æœªè§è¿‡çš„å¤„ç†")
    print("=" * 70)

    # è®­ç»ƒä¸­è§è¿‡çš„ä¸Šä¸‹æ–‡
    seen_context = (1, 2)
    print(f"\nä¸Šä¸‹æ–‡ {seen_context} (è®­ç»ƒä¸­è§è¿‡):")
    probs_seen = model_new.get_next_char_prob(seen_context)
    print(f"  åŒ…å« {len(probs_seen)} ä¸ªç¬¦å·çš„æ¦‚ç‡")
    # æ˜¾ç¤ºå‰3ä¸ªæœ€é«˜æ¦‚ç‡
    top3 = sorted(probs_seen.items(), key=lambda x: -x[1])[:3]
    for char, prob in top3:
        print(f"    ç¬¦å· {char}: {prob:.6f}")

    # è®­ç»ƒä¸­æœªè§è¿‡çš„ä¸Šä¸‹æ–‡
    unseen_context = (100, 101)
    print(f"\nä¸Šä¸‹æ–‡ {unseen_context} (è®­ç»ƒä¸­æœªè§è¿‡):")
    probs_unseen = model_new.get_next_char_prob(unseen_context)
    print(f"  è¿”å›å‡åŒ€åˆ†å¸ƒ: æ‰€æœ‰ {len(probs_unseen)} ä¸ªç¬¦å·ç­‰æ¦‚ç‡")
    print(f"  æ¯ä¸ªç¬¦å·æ¦‚ç‡: {1.0 / len(probs_unseen):.6f}")
    # éªŒè¯æ˜¯å¦å‡åŒ€
    unique_probs = set(probs_unseen.values())
    print(f"  æ˜¯å¦å‡åŒ€: {'âœ“' if len(unique_probs) == 1 else 'âœ—'}")

    # ========================================================================
    print("\nç¤ºä¾‹ 4: æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”")
    print("=" * 70)

    context = (1, 2)

    # æœ‰initial_vocabæ—¶
    probs_with = model_new.prob_distribution.get(context, {})
    print(f"\nä½¿ç”¨initial_vocab:")
    print(f"  ä¸Šä¸‹æ–‡ {context} åŒ…å« {len(probs_with)} ä¸ªç¬¦å·çš„æ¦‚ç‡")

    # æ²¡æœ‰initial_vocabæ—¶
    probs_without = model_old.prob_distribution.get(context, {})
    print(f"\nä¸ä½¿ç”¨initial_vocab:")
    print(f"  ä¸Šä¸‹æ–‡ {context} åŒ…å« {len(probs_without)} ä¸ªç¬¦å·çš„æ¦‚ç‡")

    print(f"\nå·®å¼‚: {len(probs_with) - len(probs_without)} ä¸ªé¢å¤–ç¬¦å·")

    # ========================================================================
    print("\nç¤ºä¾‹ 5: å®é™…ç¼–ç æµ‹è¯•")
    print("=" * 70)

    from arithmetic_coding import ArithmeticEncoder

    # ä½¿ç”¨æ–°æ¨¡å‹
    encoder = ArithmeticEncoder(ngram_model=model_new, bits=32)

    test_sequences = [
        [1, 2, 3],  # è®­ç»ƒä¸­è§è¿‡
        [1, 2, 100],  # åŒ…å«æœªè§ç¬¦å·
        [100, 101, 102],  # å…¨æ˜¯æœªè§ç¬¦å·
    ]

    for i, seq in enumerate(test_sequences):
        try:
            encoded = encoder.encode(seq)
            decoded = encoder.decode(encoded)
            correct = "âœ“" if decoded == seq else "âœ—"
            print(f"åºåˆ— {i} {seq}: {len(encoded)} bits {correct}")
        except Exception as e:
            print(f"åºåˆ— {i} {seq}: âœ— {e}")

    # ========================================================================
    print("\n" + "=" * 70)
    print("æ€»ç»“")
    print("=" * 70)
    print("""
âœ… å‘åå…¼å®¹ï¼š
   - ä¸æä¾›initial_vocabæ—¶ï¼Œè¡Œä¸ºä¸åŸç‰ˆå®Œå…¨ç›¸åŒ
   - ç±»åä¿æŒNGramModelï¼Œå¯ä»¥ç›´æ¥æ›¿æ¢

âœ… æ–°åŠŸèƒ½ï¼š
   - æä¾›initial_vocabå‚æ•°ï¼Œé¢„å®šä¹‰å®Œæ•´è¯æ±‡è¡¨
   - è®­ç»ƒæ•°æ®æ— éœ€ä¿®æ”¹ï¼Œä¸æ·»åŠ å ä½ç¬¦
   - è‡ªåŠ¨ä¸ºæ‰€æœ‰vocabç¬¦å·åˆ†é…æ¦‚ç‡

âœ… ä¸Šä¸‹æ–‡å¤„ç†ï¼š
   - è§è¿‡çš„ä¸Šä¸‹æ–‡ï¼šä½¿ç”¨è®­ç»ƒå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒ
   - æœªè§çš„ä¸Šä¸‹æ–‡ï¼šè¿”å›å‡åŒ€åˆ†å¸ƒï¼ˆæ‰€æœ‰ç¬¦å·ç­‰æ¦‚ç‡ï¼‰

ğŸ¯ æ¨èç”¨æ³•ï¼š
   model = NGramModel(n=3, k=0.00001, initial_vocab=set(range(2048)))
    """)