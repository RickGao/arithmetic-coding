from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Set, Optional
import pickle
import json
from pathlib import Path


class NGramModel:
    """
    N-gramè¯­è¨€æ¨¡å‹ç±»

    ç”¨äºä»ç´¢å¼•åºåˆ—ä¸­å­¦ä¹ n-gramæ¦‚ç‡åˆ†å¸ƒ
    æ”¯æŒå¼€å§‹ç¬¦ï¼ˆ-1ï¼‰å’Œç»“æŸç¬¦ï¼ˆ-2ï¼‰

    æ”¹è¿›ï¼š
    - æ”¯æŒé¢„å®šä¹‰è¯æ±‡è¡¨ï¼ˆinitial_vocabå‚æ•°ï¼‰
    - æ”¯æŒæ¨¡å‹ä¿å­˜å’ŒåŠ è½½ï¼ˆsave/loadæ–¹æ³•ï¼‰
    """

    def __init__(self,
                 n: int = 3,
                 k: float = 0.00001,
                 start_token: int = -1,
                 end_token: int = -2,
                 initial_vocab: Optional[Set[int]] = None):
        """
        åˆå§‹åŒ–N-gramæ¨¡å‹

        å‚æ•°:
            n: n-gramçš„nå€¼ï¼ˆä¾‹å¦‚ï¼Œn=3è¡¨ç¤ºtrigramï¼‰
            k: å¹³æ»‘å‚æ•°ï¼ˆåŠ æ³•å¹³æ»‘/Laplaceå¹³æ»‘ï¼‰
            start_token: å¼€å§‹ç¬¦çš„ç´¢å¼•å€¼ï¼ˆé»˜è®¤-1ï¼‰
            end_token: ç»“æŸç¬¦çš„ç´¢å¼•å€¼ï¼ˆé»˜è®¤-2ï¼‰
            initial_vocab: é¢„å®šä¹‰çš„å®Œæ•´è¯æ±‡è¡¨ï¼ˆå¯é€‰ï¼‰
                          å¦‚æœæä¾›ï¼Œä¼šç¡®ä¿è¿™äº›ç¬¦å·éƒ½åœ¨æœ€ç»ˆè¯æ±‡è¡¨ä¸­
                          å³ä½¿å®ƒä»¬åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡å‡ºç°

        ç¤ºä¾‹:
            # ä¸æŒ‡å®šinitial_vocab - å‘åå…¼å®¹ï¼Œè¡Œä¸ºå®Œå…¨ç›¸åŒ
            model = NGramModel(n=3, k=0.00001)

            # æŒ‡å®šinitial_vocab - ç¡®ä¿æ‰€æœ‰ç¬¦å·éƒ½å¯ç¼–ç 
            model = NGramModel(n=3, k=0.00001, initial_vocab=set(range(2048)))
        """
        self.n = n
        self.k = k
        self.start_token = start_token
        self.end_token = end_token
        self.initial_vocab = initial_vocab  # æ–°å¢ï¼šé¢„å®šä¹‰è¯æ±‡è¡¨
        self.vocab = set()  # è¯æ±‡è¡¨
        self.ngram_counts = Counter()  # n-gramè®¡æ•°
        self.context_counts = Counter()  # å‰n-1ä¸ªå­—ç¬¦çš„è®¡æ•°
        self.prob_distribution = {}  # æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ

    def fit(self, sequences: List[List[int]]):
        """
        è®­ç»ƒæ¨¡å‹

        å‚æ•°:
            sequences: è¾“å…¥çš„å¤šè¡Œåºåˆ—ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªlist of indices
                      ä¾‹å¦‚: [[1, 2, 3, 4], [2, 3, 4, 5, 6], ...]

        æ³¨æ„:
            - ä¼šè‡ªåŠ¨åœ¨æ¯ä¸ªåºåˆ—å¼€å¤´æ·»åŠ  n-1 ä¸ªå¼€å§‹ç¬¦ï¼ˆé»˜è®¤-1ï¼‰
            - ä¼šè‡ªåŠ¨åœ¨æ¯ä¸ªåºåˆ—ç»“å°¾æ·»åŠ  1 ä¸ªç»“æŸç¬¦ï¼ˆé»˜è®¤-2ï¼‰
            - å¦‚æœæä¾›äº†initial_vocabï¼Œä¼šç¡®ä¿æ‰€æœ‰é¢„å®šä¹‰ç¬¦å·éƒ½åœ¨è¯æ±‡è¡¨ä¸­
        """
        # é¢„å¤„ç†åºåˆ—ï¼šæ·»åŠ å¼€å§‹ç¬¦å’Œç»“æŸç¬¦
        processed_sequences = []
        for seq in sequences:
            # åœ¨å¼€å¤´æ·»åŠ  n-1 ä¸ªå¼€å§‹ç¬¦
            padded_seq = [self.start_token] * (self.n - 1) + seq + [self.end_token]
            processed_sequences.append(padded_seq)

        # åˆå¹¶æ‰€æœ‰åºåˆ—æ¥ç»Ÿè®¡ï¼ˆåŒ…æ‹¬ç‰¹æ®Šç¬¦å·ï¼‰
        all_indices = []
        for seq in processed_sequences:
            all_indices.extend(seq)

        # æ„å»ºè¯æ±‡è¡¨
        if self.initial_vocab is not None:
            # å¦‚æœæä¾›äº†é¢„å®šä¹‰è¯æ±‡è¡¨ï¼Œä½¿ç”¨å®ƒä½œä¸ºåŸºç¡€
            self.vocab = set(self.initial_vocab)
            # åŒæ—¶åŒ…å«è®­ç»ƒæ•°æ®ä¸­çš„ç¬¦å·ï¼ˆç¡®ä¿ä¸é—æ¼ï¼‰
            self.vocab.update(all_indices)
        else:
            # å¦åˆ™åªä½¿ç”¨è®­ç»ƒæ•°æ®ä¸­çš„ç¬¦å·ï¼ˆåŸå§‹è¡Œä¸ºï¼Œå‘åå…¼å®¹ï¼‰
            self.vocab = set(all_indices)

        V = len(self.vocab)

        # ç»Ÿè®¡n-gramï¼ˆåªç»Ÿè®¡å®é™…å‡ºç°çš„ï¼‰
        for seq in processed_sequences:
            for i in range(len(seq) - self.n + 1):
                ngram = tuple(seq[i:i + self.n])
                context = ngram[:-1]  # å‰n-1ä¸ªå…ƒç´ 

                self.ngram_counts[ngram] += 1
                self.context_counts[context] += 1

        # è®¡ç®—æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ
        # æ ¼å¼: {context: {next_char: probability}}
        self.prob_distribution = defaultdict(dict)

        # ä¸ºæ¯ä¸ªè§è¿‡çš„ä¸Šä¸‹æ–‡è®¡ç®—æ¦‚ç‡
        for context in self.context_counts.keys():
            context_count = self.context_counts[context]

            # å…³é”®æ”¹è¿›ï¼šä¸ºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªç¬¦å·è®¡ç®—æ¦‚ç‡
            for next_char in self.vocab:
                ngram = context + (next_char,)
                ngram_count = self.ngram_counts.get(ngram, 0)  # æœªè§è¿‡çš„ä¸º0

                # ä½¿ç”¨åŠ æ³•å¹³æ»‘ (Add-k smoothing / Laplace smoothing)
                numerator = ngram_count + self.k
                denominator = context_count + self.k * V

                probability = numerator / denominator
                self.prob_distribution[context][next_char] = probability

    def save(self, filepath: str):
        """
        ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶ï¼ˆä½¿ç”¨pickleï¼‰

        å‚æ•°:
            filepath: ä¿å­˜è·¯å¾„ï¼Œå»ºè®®ä½¿ç”¨ .pkl æ‰©å±•å
                     ä¾‹å¦‚: "model.pkl" æˆ– "models/ngram_model.pkl"

        ç¤ºä¾‹:
            model.save("ngram_model.pkl")
        """
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ‰€æœ‰éœ€è¦çš„å±æ€§
        save_dict = {
            'n': self.n,
            'k': self.k,
            'start_token': self.start_token,
            'end_token': self.end_token,
            'initial_vocab': self.initial_vocab,
            'vocab': self.vocab,
            'ngram_counts': self.ngram_counts,
            'context_counts': self.context_counts,
            'prob_distribution': dict(self.prob_distribution)
        }

        with open(filepath, 'wb') as f:
            pickle.dump(save_dict, f, protocol=pickle.HIGHEST_PROTOCOL)

        print(f"Model saved to: {filepath}")
        print(f"File size: {filepath.stat().st_size / 1024:.2f} KB")

    @classmethod
    def load(cls, filepath: str) -> 'NGramModel':
        """
        ä»æ–‡ä»¶åŠ è½½æ¨¡å‹ï¼ˆç±»æ–¹æ³•ï¼‰

        å‚æ•°:
            filepath: æ¨¡å‹æ–‡ä»¶è·¯å¾„

        è¿”å›:
            åŠ è½½çš„NGramModelå®ä¾‹

        ç¤ºä¾‹:
            model = NGramModel.load("ngram_model.pkl")
        """
        filepath = Path(filepath)

        if not filepath.exists():
            raise FileNotFoundError(f"Model does not exist: {filepath}")

        with open(filepath, 'rb') as f:
            save_dict = pickle.load(f)

        # åˆ›å»ºæ–°å®ä¾‹
        model = cls(
            n=save_dict['n'],
            k=save_dict['k'],
            start_token=save_dict['start_token'],
            end_token=save_dict['end_token'],
            initial_vocab=save_dict['initial_vocab']
        )

        # æ¢å¤è®­ç»ƒåçš„æ•°æ®
        model.vocab = save_dict['vocab']
        model.ngram_counts = save_dict['ngram_counts']
        model.context_counts = save_dict['context_counts']
        model.prob_distribution = defaultdict(dict, save_dict['prob_distribution'])

        print(f"Model Loaded: {filepath}")
        print(f"n={model.n}, k={model.k}, vocab_size={len(model.vocab)}")

        return model

    # def save_json(self, filepath: str):
    #     """
    #     ä¿å­˜æ¨¡å‹åˆ°JSONæ–‡ä»¶ï¼ˆå¯è¯»æ€§å¥½ï¼Œä½†æ–‡ä»¶è¾ƒå¤§ï¼‰
    #
    #     å‚æ•°:
    #         filepath: ä¿å­˜è·¯å¾„ï¼Œå»ºè®®ä½¿ç”¨ .json æ‰©å±•å
    #
    #     æ³¨æ„:
    #         - JSONä¸æ”¯æŒtupleä½œä¸ºkeyï¼Œä¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    #         - JSONä¸æ”¯æŒsetï¼Œä¼šè½¬æ¢ä¸ºlist
    #         - æ–‡ä»¶æ¯”pickleæ ¼å¼å¤§ï¼Œä½†äººç±»å¯è¯»
    #     """
    #     filepath = Path(filepath)
    #     filepath.parent.mkdir(parents=True, exist_ok=True)
    #
    #     # è½¬æ¢ä¸ºJSONå…¼å®¹æ ¼å¼
    #     save_dict = {
    #         'n': self.n,
    #         'k': self.k,
    #         'start_token': self.start_token,
    #         'end_token': self.end_token,
    #         'initial_vocab': list(self.initial_vocab) if self.initial_vocab else None,
    #         'vocab': list(self.vocab),
    #         'ngram_counts': {str(k): v for k, v in self.ngram_counts.items()},
    #         'context_counts': {str(k): v for k, v in self.context_counts.items()},
    #         'prob_distribution': {
    #             str(context): {str(char): prob for char, prob in dist.items()}
    #             for context, dist in self.prob_distribution.items()
    #         }
    #     }
    #
    #     with open(filepath, 'w', encoding='utf-8') as f:
    #         json.dump(save_dict, f, indent=2)
    #
    #     print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°JSON: {filepath}")
    #     print(f"   æ–‡ä»¶å¤§å°: {filepath.stat().st_size / 1024:.2f} KB")

    # @classmethod
    # def load_json(cls, filepath: str) -> 'NGramModel':
    #     """
    #     ä»JSONæ–‡ä»¶åŠ è½½æ¨¡å‹
    #
    #     å‚æ•°:
    #         filepath: JSONæ¨¡å‹æ–‡ä»¶è·¯å¾„
    #
    #     è¿”å›:
    #         åŠ è½½çš„NGramModelå®ä¾‹
    #     """
    #     filepath = Path(filepath)
    #
    #     if not filepath.exists():
    #         raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    #
    #     with open(filepath, 'r', encoding='utf-8') as f:
    #         save_dict = json.load(f)
    #
    #     # åˆ›å»ºæ–°å®ä¾‹
    #     model = cls(
    #         n=save_dict['n'],
    #         k=save_dict['k'],
    #         start_token=save_dict['start_token'],
    #         end_token=save_dict['end_token'],
    #         initial_vocab=set(save_dict['initial_vocab']) if save_dict['initial_vocab'] else None
    #     )
    #
    #     # æ¢å¤è®­ç»ƒåçš„æ•°æ®ï¼Œå°†å­—ç¬¦ä¸²keyè½¬å›tuple
    #     model.vocab = set(save_dict['vocab'])
    #     model.ngram_counts = Counter({
    #         eval(k): v for k, v in save_dict['ngram_counts'].items()
    #     })
    #     model.context_counts = Counter({
    #         eval(k): v for k, v in save_dict['context_counts'].items()
    #     })
    #     model.prob_distribution = defaultdict(dict, {
    #         eval(context): {int(char): prob for char, prob in dist.items()}
    #         for context, dist in save_dict['prob_distribution'].items()
    #     })
    #
    #     print(f"âœ… æ¨¡å‹å·²ä»JSONåŠ è½½: {filepath}")
    #     print(f"   n={model.n}, k={model.k}, vocab_size={len(model.vocab)}")
    #
    #     return model

    def get_probability_distribution(self) -> Dict[Tuple, Dict[int, float]]:
        """
        è·å–å®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒå­—å…¸

        è¿”å›:
            å­—å…¸æ ¼å¼: {(å‰n-1ä¸ªå­—ç¬¦çš„tuple): {ä¸‹ä¸€ä¸ªå­—ç¬¦: æ¦‚ç‡}}
            ä¾‹å¦‚: {(1, 2): {3: 0.5, 4: 0.3, 5: 0.2}}
        """
        return dict(self.prob_distribution)

    def get_next_char_prob(self, context: Tuple[int, ...]) -> Dict[int, float]:
        """
        ç»™å®šcontextï¼Œè¿”å›ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒ

        å‚æ•°:
            context: å‰n-1ä¸ªå­—ç¬¦ç»„æˆçš„tuple

        è¿”å›:
            å­—å…¸ {next_char: probability}

        ä¸Šä¸‹æ–‡å¤„ç†:
            - å¦‚æœcontextåœ¨è®­ç»ƒä¸­è§è¿‡ï¼šè¿”å›è®­ç»ƒå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒ
            - å¦‚æœcontextæœªè§è¿‡ï¼šè¿”å›å‡åŒ€åˆ†å¸ƒï¼ˆæ‰€æœ‰vocabç¬¦å·ç­‰æ¦‚ç‡ï¼‰
        """
        if context in self.prob_distribution:
            # ä¸Šä¸‹æ–‡è§è¿‡ï¼Œè¿”å›è®­ç»ƒçš„æ¦‚ç‡åˆ†å¸ƒ
            return self.prob_distribution[context]
        else:
            # ä¸Šä¸‹æ–‡æœªè§è¿‡ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
            V = len(self.vocab)
            return {char: 1.0 / V for char in self.vocab}

    def predict_next(self, context: Tuple[int, ...]) -> int:
        """
        ç»™å®šcontextï¼Œé¢„æµ‹æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå­—ç¬¦

        å‚æ•°:
            context: å‰n-1ä¸ªå­—ç¬¦ç»„æˆçš„tuple

        è¿”å›:
            æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå­—ç¬¦çš„index
        """
        prob_dist = self.get_next_char_prob(context)
        return max(prob_dist.items(), key=lambda x: x[1])[0]

    def get_probability(self, ngram: Tuple[int, ...]) -> float:
        """
        è®¡ç®—ç‰¹å®šn-gramçš„æ¦‚ç‡

        å‚æ•°:
            ngram: é•¿åº¦ä¸ºnçš„tuple

        è¿”å›:
            è¯¥n-gramçš„æ¡ä»¶æ¦‚ç‡
        """
        if len(ngram) != self.n:
            raise ValueError(f"ngramé•¿åº¦å¿…é¡»ä¸º{self.n}")

        context = ngram[:-1]
        next_char = ngram[-1]

        if context in self.prob_distribution:
            return self.prob_distribution[context].get(next_char, 0.0)
        else:
            # ä¸Šä¸‹æ–‡æœªè§è¿‡ï¼Œè¿”å›å‡åŒ€æ¦‚ç‡
            return 1.0 / len(self.vocab)

    def get_start_context(self) -> Tuple[int, ...]:
        """
        è·å–å¥å­å¼€å§‹æ—¶çš„contextï¼ˆn-1ä¸ªå¼€å§‹ç¬¦ï¼‰

        è¿”å›:
            å¼€å§‹contextçš„tuple
        """
        return tuple([self.start_token] * (self.n - 1))

    def is_end_token(self, token: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç»“æŸç¬¦

        å‚æ•°:
            token: è¦åˆ¤æ–­çš„token

        è¿”å›:
            æ˜¯å¦ä¸ºç»“æŸç¬¦
        """
        return token == self.end_token

    def get_model_info(self) -> Dict:
        """
        è·å–æ¨¡å‹ä¿¡æ¯æ‘˜è¦

        è¿”å›:
            åŒ…å«æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        return {
            'n': self.n,
            'k': self.k,
            'vocab_size': len(self.vocab),
            'num_unique_ngrams': len(self.ngram_counts),
            'num_unique_contexts': len(self.context_counts),
            'has_initial_vocab': self.initial_vocab is not None,
            'start_token': self.start_token,
            'end_token': self.end_token
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 70)
    print("NGramModel - ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½æ¼”ç¤º")
    print("=" * 70)

    # ç¤ºä¾‹æ•°æ®
    sequences = [
        [1, 2, 3, 4, 5],
        [2, 3, 4, 5, 6],
        [1, 2, 3, 5, 6],
        [1, 1, 2, 3, 4],
        [2, 2, 3, 4, 5],
    ]

    # ========================================================================
    print("\næ­¥éª¤ 1: è®­ç»ƒæ¨¡å‹")
    print("=" * 70)

    model = NGramModel(
        n=3,
        k=0.01,
        start_token=-1,
        end_token=-2,
        initial_vocab=set(range(100))  # é¢„å®šä¹‰è¯æ±‡è¡¨
    )
    model.fit(sequences)

    print("æ¨¡å‹ä¿¡æ¯:")
    for key, value in model.get_model_info().items():
        print(f"  {key}: {value}")

    # æµ‹è¯•é¢„æµ‹
    test_context = (1, 2)
    pred = model.predict_next(test_context)
    print(f"\né¢„æµ‹: context {test_context} -> {pred}")

    # ========================================================================
    print("\næ­¥éª¤ 2: ä¿å­˜æ¨¡å‹ï¼ˆpickleæ ¼å¼ï¼‰")
    print("=" * 70)

    model.save("ngram_model.pkl")

    # ========================================================================
    print("\næ­¥éª¤ 3: ä¿å­˜æ¨¡å‹ï¼ˆJSONæ ¼å¼ï¼‰")
    print("=" * 70)

    model.save_json("ngram_model.json")

    # ========================================================================
    print("\næ­¥éª¤ 4: åŠ è½½æ¨¡å‹ï¼ˆpickleæ ¼å¼ï¼‰")
    print("=" * 70)

    loaded_model = NGramModel.load("ngram_model.pkl")

    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    pred_loaded = loaded_model.predict_next(test_context)
    print(f"\nåŠ è½½åé¢„æµ‹: context {test_context} -> {pred_loaded}")
    print(f"é¢„æµ‹æ˜¯å¦ä¸€è‡´: {'âœ“' if pred == pred_loaded else 'âœ—'}")

    # ========================================================================
    print("\næ­¥éª¤ 5: åŠ è½½æ¨¡å‹ï¼ˆJSONæ ¼å¼ï¼‰")
    print("=" * 70)

    loaded_model_json = NGramModel.load_json("ngram_model.json")

    pred_json = loaded_model_json.predict_next(test_context)
    print(f"\nJSONåŠ è½½åé¢„æµ‹: context {test_context} -> {pred_json}")
    print(f"é¢„æµ‹æ˜¯å¦ä¸€è‡´: {'âœ“' if pred == pred_json else 'âœ—'}")

    # ========================================================================
    print("\næ­¥éª¤ 6: å¯¹æ¯”æ¦‚ç‡åˆ†å¸ƒ")
    print("=" * 70)

    prob_original = model.get_next_char_prob(test_context)
    prob_loaded = loaded_model.get_next_char_prob(test_context)
    prob_json = loaded_model_json.get_next_char_prob(test_context)

    print(f"\nä¸Šä¸‹æ–‡ {test_context} çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæ˜¾ç¤ºå‰5ä¸ªï¼‰:")
    top5_original = sorted(prob_original.items(), key=lambda x: -x[1])[:5]

    print("\nåŸå§‹æ¨¡å‹:")
    for char, prob in top5_original:
        print(f"  {char}: {prob:.6f}")

    print("\nPickleåŠ è½½:")
    for char, prob in sorted(prob_loaded.items(), key=lambda x: -x[1])[:5]:
        print(f"  {char}: {prob:.6f}")

    print("\nJSONåŠ è½½:")
    for char, prob in sorted(prob_json.items(), key=lambda x: -x[1])[:5]:
        print(f"  {char}: {prob:.6f}")

    # æ£€æŸ¥æ¦‚ç‡æ˜¯å¦å®Œå…¨ä¸€è‡´
    prob_match = all(
        abs(prob_original.get(char, 0) - prob_loaded.get(char, 0)) < 1e-10
        for char in set(prob_original.keys()) | set(prob_loaded.keys())
    )
    print(f"\næ¦‚ç‡å®Œå…¨ä¸€è‡´: {'âœ“' if prob_match else 'âœ—'}")

    # ========================================================================
    print("\n" + "=" * 70)
    print("æ€»ç»“")
    print("=" * 70)
    print("""
âœ… ä¸¤ç§ä¿å­˜æ ¼å¼ï¼š
   1. Pickleæ ¼å¼ï¼ˆæ¨èï¼‰ï¼š
      - æ–‡ä»¶å°ï¼Œé€Ÿåº¦å¿«
      - å®Œå…¨ä¿ç•™æ‰€æœ‰Pythonå¯¹è±¡
      - ä½¿ç”¨: model.save("model.pkl")
      - åŠ è½½: model = NGramModel.load("model.pkl")

   2. JSONæ ¼å¼ï¼š
      - äººç±»å¯è¯»
      - è·¨è¯­è¨€å…¼å®¹
      - æ–‡ä»¶è¾ƒå¤§
      - ä½¿ç”¨: model.save_json("model.json")
      - åŠ è½½: model = NGramModel.load_json("model.json")

âœ… ä½¿ç”¨åœºæ™¯ï¼š
   - ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨pickleï¼Œæ•ˆç‡é«˜
   - è°ƒè¯•/æ£€æŸ¥ï¼šä½¿ç”¨JSONï¼Œå¯ä»¥æ‰‹åŠ¨æŸ¥çœ‹
   - è·¨å¹³å°ï¼šä½¿ç”¨JSONï¼Œæ›´é€šç”¨

ğŸ¯ æ¨èç”¨æ³•ï¼š
   # è®­ç»ƒ
   model = NGramModel(n=3, k=0.00001, initial_vocab=set(range(2048)))
   model.fit(sequences)
   model.save("models/ngram_model.pkl")

   # åŠ è½½
   model = NGramModel.load("models/ngram_model.pkl")
    """)